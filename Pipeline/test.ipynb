{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from fastapi.responses import RedirectResponse\n",
    "from typing import Dict,Any\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.schema.document import Document\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from unstructured.partition.auto import partition\n",
    "import os\n",
    "import uuid\n",
    "import subprocess\n",
    "import torch\n",
    "import base64\n",
    "import time\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "app = FastAPI()\n",
    "\n",
    "origins = [\n",
    "    \"http://localhost\",\n",
    "    \"http://localhost:8000\"\n",
    "    \"http://localhost:8080\",\n",
    "]\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=origins,\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2871358/722885061.py:3: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embedding_function=HuggingFaceEmbeddings(),\n"
     ]
    }
   ],
   "source": [
    "vectorstore = Chroma(\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding_function=HuggingFaceEmbeddings(),\n",
    ")\n",
    "\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2871358/1619034594.py:1: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"llama3.2\", stop = [\"###\", \"{\", \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\"], system = \"If I tell you that I am not well or have any medical problem, analyse my conditions carefully, and remember any information about my symptoms or medical history. Keep asking follow-up questions about the symptoms and any other information that may help you to narrow down at a differential diagnosis. Do not ask more than 2 questions at a time. If and only if you are confident about it, provide me with a list of possible diagnoses, three or four at maximum, ranked by likelihood, and a brief explanation of your reasoning. Keep asking questions otherwise, not more than 2 at a time.\",num_gpu=1)\n",
      "/tmp/ipykernel_2871358/1619034594.py:4: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(llava(\"Hi\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello! How can I help you today? Is there something specific you would like to know or discuss? I'm here to provide information and answer questions to the best of my ability. \n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(model=\"llama3.2\", stop = [\"###\", \"{\", \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\"], system = \"If I tell you that I am not well or have any medical problem, analyse my conditions carefully, and remember any information about my symptoms or medical history. Keep asking follow-up questions about the symptoms and any other information that may help you to narrow down at a differential diagnosis. Do not ask more than 2 questions at a time. If and only if you are confident about it, provide me with a list of possible diagnoses, three or four at maximum, ranked by likelihood, and a brief explanation of your reasoning. Keep asking questions otherwise, not more than 2 at a time.\",num_gpu=1)\n",
    "\n",
    "llava = Ollama(model=\"llava:7b-v1.6-mistral-q2_K\")\n",
    "print(llava(\"Hi\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def image_to_base64(image_path):\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            encoded_string = base64.b64encode(image_file.read())\n",
    "            return encoded_string.decode(\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return RedirectResponse(url=\"/docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/upload_img\")\n",
    "async def up_img(inp)-> Dict[str,str]:\n",
    "    try:\n",
    "        cleaned_img_summary = []\n",
    "        filename = inp\n",
    "        \n",
    "    # Iterate over matched file paths\n",
    "        for img in [filename]:\n",
    "            RES = llava(prompt=\"Provide a concise, factual summary of the image, capturing all the key visual elements and details you observe. Avoid speculative or imaginative descriptions not directly supported by the contents of the image. Focus on objectively describing what is present in the image without introducing any external information or ideas. Your summary should be grounded solely in the visual information provided.\" ,images=[str(image_to_base64(f\"./{img}\"))])\n",
    "            cleaned_img_summary.append(RES)\n",
    "        # print(cleaned_img_summary)\n",
    "        \n",
    "        img_ids = [str(uuid.uuid4()) for _ in cleaned_img_summary]\n",
    "        summary_img = [\n",
    "            Document(page_content=s, metadata={id_key: img_ids[i]})\n",
    "            for i, s in enumerate(cleaned_img_summary)\n",
    "        ]\n",
    "\n",
    "        retriever.vectorstore.add_documents(summary_img)\n",
    "        retriever.docstore.mset(\n",
    "            list(zip(img_ids, cleaned_img_summary))\n",
    "        )\n",
    "        \n",
    "        return {\"message\":\"200\"}\n",
    "    \n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        return {\"message\":\"404\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/generate\")\n",
    "async def generate_output(inp: str) -> Dict[str, str]:\n",
    "    s = \"You Are my personal doctor. You have to Remember my symptoms antecendants past history and try to ask me follow up questions whenever I tell you that I am not well\"\n",
    "\n",
    "# Prompt template\n",
    "    template = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. {{ if .}}### Instruction: If I tell you that I am not well or have any medical problem, analyse my conditions carefully, and remember any information about my symptoms or medical history. Keep asking follow-up questions about the symptoms and any other information that may help you to narrow down at a differential diagnosis. Do not ask more than 2 questions at a time. If and only if you are confident about it, provide me with a list of possible diagnoses, three or four at maximum, ranked by likelihood, and a brief explanation of your reasoning. Keep asking questions otherwise, not more than 2 at a time. You also have the following context, which can include text and tables to answer my QUESTION {{ .System }}{{ end }} {{ if .Prompt }}{context}\n",
    "                    Question: {question}{{ .Prompt }}{{ end }} ### Response:\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    txt = chain.invoke(inp)\n",
    "\n",
    "    return {\"message\":txt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/upload\")\n",
    "async def post_pdf() -> Dict[str,str]:\n",
    "    try:\n",
    "        print(\"here1\")\n",
    "        filename = \"uploaded_file.pdf\"\n",
    "        path = \"./\"\n",
    "\n",
    "        # Extract images, tables, and chunk text\n",
    "        raw_pdf_elements = partition_pdf(\n",
    "            filename = path + filename,\n",
    "            extract_images_in_pdf=True,\n",
    "            infer_table_structure=True,\n",
    "            chunking_strategy=\"by_title\",\n",
    "            max_characters=4000,\n",
    "            new_after_n_chars=3800,\n",
    "            combine_text_under_n_chars=2000,\n",
    "            image_output_dir_path=path,\n",
    "        )\n",
    "        category_counts = {}\n",
    "        \n",
    "        print(\"Safe1\")\n",
    "\n",
    "        for element in raw_pdf_elements:\n",
    "            category = str(type(element))\n",
    "            if category in category_counts:\n",
    "                category_counts[category] += 1\n",
    "            else:\n",
    "                category_counts[category] = 1\n",
    "\n",
    "        # Unique_categories will have unique elements\n",
    "        # TableChunk if Table > max chars set above\n",
    "        unique_categories = set(category_counts.keys())\n",
    "        category_counts\n",
    "        class Element(BaseModel):\n",
    "            type: str\n",
    "            text: Any\n",
    "\n",
    "        # Categorize by type\n",
    "        categorized_elements = []\n",
    "        for element in raw_pdf_elements:\n",
    "            if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "                categorized_elements.append(Element(type=\"table\", text=str(element)))\n",
    "            elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "                categorized_elements.append(Element(type=\"text\", text=str(element)))\n",
    "\n",
    "        print(\"safe2\")\n",
    "        # Tables\n",
    "        table_elements = [e for e in categorized_elements if e.type == \"table\"]\n",
    "        # print(len(table_elements))\n",
    "\n",
    "        # Text\n",
    "        text_elements = [e for e in categorized_elements if e.type == \"text\"]\n",
    "        # print(len(text_elements))\n",
    "\n",
    "        # Prompt\n",
    "        prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text. \\\n",
    "        Give a concise summary of the table or text. Table or text chunk: {element} \"\"\"\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "        summarize_chain = {\"element\": lambda x: x} | prompt | llm | StrOutputParser()\n",
    "\n",
    "        texts = [i.text for i in text_elements if i.text != \"\"]\n",
    "        text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})\n",
    "        print(text_summaries)\n",
    "        # Apply to tables\n",
    "        tables = [i.text for i in table_elements]\n",
    "        table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})\n",
    "\n",
    "        # Add texts\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "        summary_texts = [\n",
    "            Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "            for i, s in enumerate(text_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_texts)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, texts)))\n",
    "\n",
    "        # Add tables\n",
    "        table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "        summary_tables = [\n",
    "            Document(page_content=s, metadata={id_key: table_ids[i]})\n",
    "            for i, s in enumerate(table_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_tables)\n",
    "        retriever.docstore.mset(list(zip(table_ids, tables)))\n",
    "\n",
    "        subprocess.Popen(\"ollama serve\", shell=True)\n",
    "        #!sleep 10\n",
    "\n",
    "        IMG_DIR = './'\n",
    "\n",
    "        # Use glob to match file paths\n",
    "        image_files = glob.glob(f\"{IMG_DIR}*.jpg\")\n",
    "        # print(image_files)\n",
    "        cleaned_img_summary = []\n",
    "\n",
    "        # Iterate over matched file paths\n",
    "        for img in image_files:\n",
    "            # Perform your operation here\n",
    "            RES = llava(prompt=\"Provide a concise, factual summary of the image, capturing all the key visual elements and details you observe. Avoid speculative or imaginative descriptions not directly supported by the contents of the image. Focus on objectively describing what is present in the image without introducing any external information or ideas. Your summary should be grounded solely in the visual information provided.\" ,images=[str(image_to_base64(img))])\n",
    "            cleaned_img_summary.append(RES)\n",
    "            \n",
    "        img_ids = [str(uuid.uuid4()) for _ in cleaned_img_summary]\n",
    "        summary_img = [\n",
    "            Document(page_content=s, metadata={id_key: img_ids[i]})\n",
    "            for i, s in enumerate(cleaned_img_summary)\n",
    "        ]\n",
    "        print(cleaned_img_summary)\n",
    "        \n",
    "        retriever.vectorstore.add_documents(summary_img)\n",
    "        retriever.docstore.mset(\n",
    "            list(zip(img_ids, cleaned_img_summary))\n",
    "        )\n",
    "        return {\"message\":\"200\"}\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return {\"message\":\"404\"}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opdx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
